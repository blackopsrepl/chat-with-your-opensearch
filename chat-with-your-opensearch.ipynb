{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install IProgress ipywidgets langchain_community langchain_openai opensearch_py tqdm unstructured networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start local opensearch node by running:\n",
    "```bash podman run -d -p 9200:9200 -p 9600:9600 --name elastic -e \"discovery.type=single-node\" -e \"plugins.security.disabled=true\" opensearchproject/opensearch:latest```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base Chatbot - Langchain + Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"OpenAI API Key: \")\n",
    "es_url = getpass(\"Opensearch URL: \")\n",
    "index_name = getpass(\"Opensearch Index Name: \")\n",
    "hf = OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-ada-002\")\n",
    "db = OpenSearchVectorSearch(embedding_function=hf, opensearch_url=es_url, index_name=index_name, ssl_verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('pages', glob=\"./*.txt\", show_progress=True, recursive=True)\n",
    "data = loader.load()\n",
    "batchtext = []\n",
    "count = 0\n",
    "for doc in tqdm(data, desc=\"Processing documents\"):\n",
    "    # assuming data is a list of documents, containing paragraphs separated by \\n\n",
    "    paragraphs = doc.page_content.split('\\n')\n",
    "    for p in paragraphs:\n",
    "        batchtext.append(p)\n",
    "        count += 1\n",
    "print(f\"Total paragraphs: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_paragraphs = len(batchtext)\n",
    "pbar = tqdm(total=total_paragraphs, desc=\"Embedding paragraphs\")\n",
    "chunk_size = 1000\n",
    "for i in range(0, total_paragraphs, chunk_size):\n",
    "    chunk = batchtext[i:i+chunk_size]\n",
    "    db.from_texts(chunk, embedding=hf, opensearch_url=es_url, index_name=index_name, bulk_size=4000)\n",
    "    pbar.update(len(chunk))\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Sinapto srl technology consulting knowledge base\"\n",
    "template_informed = \"\"\"\n",
    "Sono un assistente e custode di una knowledge base testuale. Rispondo alle domande basandomi sul contesto fornito. Se non conosco la risposta, dico che non lo so.\n",
    "Conosco il contesto: {context}\n",
    "Quando mi viene chiesto: {question}\n",
    "la mia risposta, basata solo sulle informazioni del contesto, Ã¨: \"\"\"\n",
    "prompt_informed = PromptTemplate(template=template_informed, input_variables=[\"context\", \"question\"])\n",
    "llm_chain_informed = LLMChain(prompt=prompt_informed, llm=ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key))\n",
    "\n",
    "def ask_a_question(question):\n",
    "    # composing context informed prompt and response from retrieved similarity search results\n",
    "    similar_docs = db.similarity_search(question, k=1000)\n",
    "    informed_context = (lambda docs: \"\".join(doc.page_content for doc in docs))(similar_docs)\n",
    "    informed_response = (lambda context, question: llm_chain_informed.run(context=context, question=question))(informed_context, question)\n",
    "    return informed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\"{topic}\": advanced search')\n",
    "while True:\n",
    "    question = input(\"User Question >> \")\n",
    "    response = ask_a_question(question)\n",
    "    print(f\"\\tQuestion: {question}\")\n",
    "    print(f\"\\tAnswer  : {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinsearch04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
